{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity Models\n",
    "This script builds models to assess the similarity between two pieces of text. This will be used in the SimplifyIT app to ensure that generated simplified text accurately captures the meaning of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for evaluating text similarity\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk resources for tokenization and part of speech tagging\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Locations of clean and raw data\n",
    "ClnDat = '../data_clean/'\n",
    "RawDat = '../data_raw/'\n",
    "\n",
    "# Location to save trained models\n",
    "TrnMod = '../trained_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to process data for modeling\n",
    "\n",
    "# Function takes a list of words and returns a list in which all stop words are removed\n",
    "def remove_stop_words(wordlist):\n",
    "    # Get all English stop words\n",
    "    stops = set(stopwords.words(\"english\"))  \n",
    "    nostops = [w for w in wordlist if w not in stops]\n",
    "    return nostops\n",
    "\n",
    "# Function takes a list of words and returns a list of word stems\n",
    "def stem_words(wordlist):\n",
    "    # Initialize object to stem words\n",
    "    ps = PorterStemmer()\n",
    "    stems = [ps.stem(w) for w in wordlist]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wikipedia sentence text\n",
    "SentDF = pd.read_csv(ClnDat + 'wiki_sentence.csv').drop(columns = ['same'])\n",
    "\n",
    "# Melt data\n",
    "SentDF = SentDF.melt(id_vars = ['sent_id', 'topic'])\n",
    "SentDF.columns = ['name', 'topic', 'level', 'text']\n",
    "\n",
    "# Subset data\n",
    "SentDF = SentDF[['name', 'level', 'text']]\n",
    "\n",
    "# Remove numbers and symbols and convert string to lower\n",
    "SentDF['text_c'] = SentDF['text'].str.replace(r'[^a-zA-Z\\s+]', '').str.lower()\n",
    "\n",
    "# Replace double spaces with single space\n",
    "SentDF['text_c'] = SentDF['text_c'].str.replace(r'\\s+\\s+', ' ')\n",
    "\n",
    "# Tokenize text entries\n",
    "SentDF['text_c'] = SentDF['text_c'].apply(word_tokenize)\n",
    "\n",
    "# Stem text entries\n",
    "SentDF['text_c'] = SentDF['text_c'].apply(stem_words)\n",
    "\n",
    "# Remove stop words\n",
    "SentDF['text_c'] = SentDF['text_c'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag and label each document/sentence in the corpus\n",
    "tagged_data = [TaggedDocument(words = doc, tags = [i]) for i, doc in enumerate(SentDF['text_c'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train doc2vec model on wikipedia sentence data set\n",
    "\n",
    "# Specify model parameters\n",
    "max_epochs = 10\n",
    "vec_size = 50\n",
    "alpha = 0.025\n",
    "\n",
    "# Define model\n",
    "model = Doc2Vec(size = vec_size,\n",
    "                alpha = alpha, \n",
    "                min_alpha = 0.00025,\n",
    "                min_count = 1,\n",
    "                dm = 1)\n",
    "\n",
    "# Add vocabulary to model\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "# Save model to trained_models directory\n",
    "model.save(TrnMod + \"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = Doc2Vec.load(TrnMod + \"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentComp = SentDF[SentDF['name'].isin([3, 4, 5, 6])].reset_index()['text_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between two entries\n",
    "model.wv.n_similarity(SentComp[1], SentComp[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentDF = pd.read_csv(ClnDat + 'OSE_TextFeat.csv')\n",
    "\n",
    "SentDF['tok_adv'] = SentDF['text'].apply(word_tokenize)\n",
    "SentDF['tok_adv'] = [i for i in SentDF['tok_adv']]\n",
    "# SentDF['tok_ele'] = SentDF['text_ele'].apply(word_tokenize)\n",
    "# SentDF['pos_adv'] = SentDF['tok_adv'].apply(nltk.pos_tag)\n",
    "SentDF.head()\n",
    "\n",
    "# SentDF['text_adv'] = SentDF['text_adv'].str.replace(r'[^a-zA-Z\\s+]', '').str.replace(r'\\s+\\s+', ' ').str.lower()\n",
    "\n",
    "# SentDF['text_ele'] = SentDF['text_ele'].str.replace(r'[^a-zA-Z\\s+]', '').str.replace(r'\\s+\\s+', ' ').str.lower()\n",
    "\n",
    "# SentDF['tok_adv'] = [t.split(' ') for t in SentDF['text_adv']]\n",
    "# SentDF['tok_adv'] = SentDF['tok_adv'].apply(list)\n",
    "# SentDF['tok_ele'] = [t.split(' ') for t in SentDF['text_ele']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = SentDF['text_c']\n",
    "model = Word2Vec(sentences, min_count=5, window=5, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcheck = 'government'\n",
    "model.wv[wordcheck]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similar_by_word(wordcheck, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
