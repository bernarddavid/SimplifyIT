{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data: Cleaning and EDA\n",
    "The Wikipedia corpus contains entries from Wikipedia and their counterparts in the simplified \"English\" Wikipedia. These entries can be aligned by topic (e.g., the entire entry) or by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import textstat\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets option to display all text in pandas dataframes\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Specify File Locations\n",
    "RawDat = '../data_raw/'\n",
    "ClnDat = '../data_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify whether to load in doc or sentence aligned Wikipedia data\n",
    "Alignment = 'sentence' # Can be either 'doc' or 'sentence'\n",
    "if Alignment == 'doc':\n",
    "    FileExt = '_d.txt'\n",
    "if Alignment == 'sentence':\n",
    "    FileExt = '_s.txt'\n",
    "\n",
    "WikiDF = pd.DataFrame() # Initialize pandas dataframe for data import\n",
    "for level in ['normal', 'simple']:\n",
    "    \n",
    "    FileLoc = RawDat+'/WikiData/'+Alignment+'_aligned/'+level+FileExt\n",
    "    with open(FileLoc, 'rb') as dataload:\n",
    "        result = chardet.detect(dataload.read(10000))\n",
    "\n",
    "    # Load Normal and Simplified English Wikipedia datasets\n",
    "    WikiDF0 = pd.read_csv(FileLoc, \n",
    "                          sep = '\\t', \n",
    "                          encoding = result['encoding'],\n",
    "                          header =  None,\n",
    "                          names = ['topic', 'paragraph_num', 'text'])\n",
    "    \n",
    "    WikiDF0['level'] = level\n",
    "    WikiDF0['sent_id'] = list(range(1, int(len(WikiDF0.index))+1))\n",
    "    \n",
    "    WikiDF = WikiDF.append(WikiDF0)\n",
    "    \n",
    "# If Alignment at sentence aligned, pivot by \"sentence number\" to create rows\n",
    "if Alignment == 'sentence':\n",
    "    WikiDF = WikiDF.pivot(index = ['sent_id', 'topic'], columns = ['level'], values = ['text']).reset_index()\n",
    "    # Flatten column index of pivot table\n",
    "    WikiDF.columns = WikiDF.columns.map('_'.join).str.strip('_')\n",
    "    \n",
    "    # Flag and remove sentences that are perfectly aligned already\n",
    "    WikiDF['same'] = np.where(WikiDF['text_normal'] == WikiDF['text_simple'], 1, 0)\n",
    "    WikiDF = WikiDF[WikiDF['same'] != 1].reset_index(drop = True)\n",
    "    WikiDF.columns = ['sent_id', 'topic', 'normal', 'simple', 'same']\n",
    "    \n",
    "    # Replace non ascii characters\n",
    "    WikiDF['normal'] = WikiDF['normal'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "    WikiDF['simple'] = WikiDF['simple'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "    # Replace LRB and RRB symbols\n",
    "    WikiDF['normal'] = WikiDF['normal'].str.replace(r'-LRB-|-RRB-', '')\n",
    "    WikiDF['simple'] = WikiDF['simple'].str.replace(r'-LRB-|-RRB-', '')\n",
    "\n",
    "    # Export WikiDF to csv\n",
    "    WikiDF.to_csv(ClnDat+'wiki_'+Alignment+'.csv', header = True, index = False)\n",
    "    \n",
    "# If Alignment at document aligned, merge sentences into paragraph and paragraphs into single file for doc\n",
    "if Alignment == 'doc':\n",
    "    WikiDF = WikiDF.groupby(['topic', 'type', 'paragraph_num'], as_index = False).agg({'text': ' '.join})\n",
    "    WikiDF = WikiDF.groupby(['topic', 'type'], as_index = False).agg({'text': '\\n '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text features for Wiki corpus aligned at document level\n",
    "if Alignment == 'doc':\n",
    "    # For exploratory data analysis, get random sample of topics\n",
    "    RandTopics = pd.DataFrame(WikiDF['topic'].unique()).sample(1000)\n",
    "    RandTopics.columns = ['topic']\n",
    "\n",
    "    # Subset Wikipedia dataframe to random sample of topics\n",
    "    WikiDF_sub = WikiDF[WikiDF['topic'].isin(RandTopics['topic'])].reset_index()\n",
    "    len(WikiDF_sub.index)\n",
    "    \n",
    "    # Compute text readability score for subset\n",
    "    WikiDF_sub['text'] = WikiDF_sub['text'].apply(str) # Turn text to string\n",
    "    WikiDF_sub['fkg_score'] = WikiDF_sub['text'].apply(textstat.flesch_kincaid_grade)\n",
    "    WikiDF_sub['flesch_read'] = WikiDF_sub['text'].apply(textstat.flesch_reading_ease)\n",
    "    WikiDF_sub['fog_score'] = WikiDF_sub['text'].apply(textstat.gunning_fog)\n",
    "    WikiDF_sub['ari_score'] = WikiDF_sub['text'].apply(textstat.automated_readability_index)\n",
    "    WikiDF_sub['cli_score'] = WikiDF_sub['text'].apply(textstat.coleman_liau_index)\n",
    "    WikiDF_sub['lwf_score'] = WikiDF_sub['text'].apply(textstat.linsear_write_formula)\n",
    "    WikiDF_sub['dcr_score'] = WikiDF_sub['text'].apply(textstat.dale_chall_readability_score)\n",
    "    WikiDF_sub['consensus'] = WikiDF_sub['text'].apply(textstat.text_standard)\n",
    "    WikiDF_sub['n_sentences'] = WikiDF_sub['text'].apply(textstat.sentence_count)\n",
    "    WikiDF_sub['n_syllables'] = WikiDF_sub['text'].apply(textstat.syllable_count)\n",
    "    WikiDF_sub['n_lexicon'] = WikiDF_sub['text'].apply(textstat.lexicon_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WikiDF_sub.groupby('type')['lwf_score'].plot(kind = 'hist', legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiDF_sub.groupby('type')['fog_score'].plot(kind = 'hist', legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiDF_sub['topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub2 = WikiDF_sub[WikiDF_sub['topic'] == '1992 Pacific hurricane season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub2[['topic', 'type', 'text', 'n_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(WikiDF_sub, index = ['consensus'], columns = ['type'], values = ['text'], aggfunc = 'count').reset_index().plot.bar(x = 'consensus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(WikiDF_sub['text'][49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WikiDF_sub.boxplot(column = 'fog_score', by = 'type')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
