{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneStopEnglish: Data Cleaning and EDA\n",
    "This script imports the One Stop English corpus, which consists of leveled text entries (beginner, intermediate, advanced) aligned to the same topic. Several features of the text entries are calculated using the `textstat` and `spacy` packages. These numerical features are then exported to a csv so they can be incoporated in a machine learning classification model that evaluates text complexity. Additional csv files are exported to finetune simpleGPT2 models for text generation. A list of the data prepared in this script is included below:\n",
    "1. One column CSV files containing only text entries to be used to train NLP (simpleGPT2) models (**NOTE**, these can be created at the \"sentence\" and \"passage\" levels):\n",
    "    1. _OSE\\_adv\\_int_: Matched advanced and intermediate texts, difficulty separated by indicator.\n",
    "    1. _OSE\\_adv\\_ele_: Matched advanced and elementary texts, difficulty separated by indicator.\n",
    "    1. _OSE\\_int\\_ele_: Matched intermediate adn elementary texts, difficulty separated by indicator.\n",
    "1. A dataframe containing numerical characteristics about text entries to be used in training ML models to evaluate text complexity.\n",
    "    1. _OSE\\_TextFeat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import textstat\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function splits text, t, into sentences\n",
    "def sent_break(t):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(t)\n",
    "    return doc.sents\n",
    "\n",
    "# Function counts the number of words in text t\n",
    "def word_count(t):\n",
    "    sents = sent_break(t)\n",
    "    n_words = 0\n",
    "    for s in sents:\n",
    "        n_words += len([token for token in s])\n",
    "    return n_words\n",
    "\n",
    "# Function counts the number of sentences in text t\n",
    "def sent_count(t):\n",
    "    sents = sent_break(t)\n",
    "    return len(list(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets option to display all text in pandas dataframes\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Change working directory to main project directory\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Data Type: Either \"TextByReadingLevel\" or \"SentenceAligned\"\n",
    "DatType = 'TextByReadingLevel/'\n",
    "\n",
    "# Get file path for OneStopEnglish Corpus from individual TXT files.\n",
    "RawDat = os.getcwd() + '/data_raw/OneStopEng/' + DatType\n",
    "\n",
    "# Set location to save clean data\n",
    "ClnDat = os.getcwd() + '/data_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason there are two \"Int\" files--do not import the first, as it contains errors\n",
    "SubDir = [join(RawDat, f)+'/' for f in listdir(RawDat) if f != 'Int-Txt']\n",
    "\n",
    "# Load all texts into pandas data frame\n",
    "TextDF = pd.DataFrame()\n",
    "\n",
    "for s in SubDir:\n",
    "\n",
    "    TextFiles = listdir(s)\n",
    "\n",
    "    for t in TextFiles:\n",
    "        # There is a hidden .ds_store file that should be skipped when importing data\n",
    "        if t == '.DS_Store':\n",
    "            pass\n",
    "        else:\n",
    "            with open(s + t, 'rb') as rawdat:\n",
    "                result = chardet.detect(rawdat.read(1000))\n",
    "\n",
    "                \n",
    "            file = open(s + t, 'r', encoding = result['encoding'])\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Append text data and file name to dataframe\n",
    "            TextDF = TextDF.append({'file_nm' : t,\n",
    "                                    'text' : text},\n",
    "                                   ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate name and level of text\n",
    "TextDF['name'] = TextDF['file_nm'].str[:-8]\n",
    "TextDF['level'] = TextDF['file_nm'].str[-7:-4]\n",
    "TextDF['text'] = TextDF['text'].apply(str)\n",
    "\n",
    "# TextDF.groupby(['level'])['text'].count()\n",
    "\n",
    "# Intermediate text is labeled--this should be removed\n",
    "# TextDF['text'] = TextDF['text'].map(lambda x: x.lstrip('Intermediate'))\n",
    "# TextDF['text'] = TextDF['text'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot text to level\n",
    "TextDF_w = TextDF.pivot(index = ['name'], columns = ['level'], values = ['text']).reset_index()\n",
    "\n",
    "# Flatten column index of pivot table\n",
    "TextDF_w.columns = TextDF_w.columns.map('_'.join).str.strip('_')\n",
    "\n",
    "# Match text pairs accross levels to train GPT2 Model\n",
    "TextDF_w['adv_int'] = TextDF_w['text_adv'] + '\\n |<endoflevelone>| \\n' + TextDF_w['text_int']\n",
    "TextDF_w['adv_ele'] = TextDF_w['text_adv'] + '\\n |<endoflevelone>| \\n' + TextDF_w['text_ele']\n",
    "TextDF_w['int_ele'] = TextDF_w['text_int'] + '\\n |<endoflevelone>| \\n' + TextDF_w['text_ele']\n",
    "\n",
    "# Export matched text columns to csv file to finetune SimpleGPT2, need to specify encoding or odd figures are saved\n",
    "# One column must be included for file to successfully be read into SimpleGPT2\n",
    "for lev in ['adv_int', 'adv_ele', 'int_ele']:\n",
    "    TextDF_w[lev] = TextDF_w[lev].apply(str)\n",
    "    TextDF_w[lev].to_csv(ClnDat+'OSE_'+lev+'.csv', encoding = 'utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text (text_c) by lowering, removing numbers, removing symbols, and reducing to single space\n",
    "TextDF['text_c'] = TextDF['text'].str.lower().str.replace(r'[^a-zA-Z\\s]', '')\n",
    "TextDF['text_c'] = TextDF['text_c'].str.replace(r'\\n', ' ').str.replace(r'\\s+\\s', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is separate from the other text statistic computations because it takes a longer time to run\n",
    "# Compute text difficulty using textstat\n",
    "TextDF['difficulty'] = TextDF['text'].apply(textstat.flesch_reading_ease)\n",
    "\n",
    "# Compute sentence length\n",
    "TextDF['n_sent'] = TextDF['text'].apply(sent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute additional text statistics\n",
    "TextDF['n_syll'] = TextDF['text'].apply(textstat.syllable_count) # Number of syllables\n",
    "TextDF['n_lex'] = TextDF['text'].apply(textstat.lexicon_count) # Number of words\n",
    "TextDF['lex_sent'] = TextDF['n_lex']/TextDF['n_sent'] # Word to sentence ratio\n",
    "TextDF['syll_lex'] = TextDF['n_syll']/TextDF['n_lex'] # Syllable to word ratio\n",
    "\n",
    "# Save text statistc data to csv\n",
    "TextDF.to_csv(ClnDat+'OSE_TextFeat.csv', encoding = 'utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDF.groupby('level')['n_sent'].plot(kind = 'hist', legend = True, alpha = 0.5, \n",
    "                                       title = 'Number of Sentences by Text Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDF.groupby('level')['n_syll'].plot(kind = 'hist', legend = True, alpha = 0.5, \n",
    "                                       title = 'Number of Syllables by Text Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDF.groupby('level')['n_lex'].plot(kind = 'hist', legend = True, alpha = 0.5,\n",
    "                                      title = 'Number of Words by Text Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDF.groupby('level')['lex_sent'].plot(kind = 'hist', legend = True, alpha = 0.5,\n",
    "                                         title = 'Word to Sentence Ratio by Text Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDF.groupby('level')['syll_lex'].plot(kind = 'hist', legend = True, alpha = 0.5, \n",
    "                                         title = 'Syllable to Word Ratio by Text Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDF.groupby('level')['difficulty'].plot(kind = 'hist', legend = True, alpha = 0.5, \n",
    "                                           title = 'Readability Score by Text Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned sentence aligned files for text generation analysis\n",
    "\n",
    "# Data Type: Either \"TextByReadingLevel\" or \"SentenceAligned\"\n",
    "DatType = 'SentenceAligned/'\n",
    "\n",
    "# Get file path for OneStopEnglish Corpus from individual TXT files.\n",
    "RawDat = os.getcwd() + '/data_raw/OneStopEng/' + DatType\n",
    "\n",
    "t_files = ['ADV-ELE.txt', 'ADV-INT.txt', 'ELE-INT.txt']\n",
    "t = 'ADV-ELE.txt'\n",
    "\n",
    "l1 = 'adv'\n",
    "l2 = 'ele'\n",
    "with open(RawDat + t, 'rb') as rawdat:\n",
    "    result = chardet.detect(rawdat.read(10000))\n",
    "    \n",
    "file = open(RawDat + t, 'r', encoding = result['encoding'])\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text = text.split('\\n')\n",
    "\n",
    "TextDF = pd.DataFrame(text, columns = ['text'])\n",
    "TextDF = TextDF[~TextDF['text'].isin(['*******', '', ' '])].reset_index().drop(columns = 'index')\n",
    "TextDF['level'] = [l1, l2]*int(len(TextDF.index)/2)\n",
    "TextDF['label'] = list(np.repeat(list(range(1, int(len(TextDF.index)/2)+1)), 2))\n",
    "TextDF['label'] = 's' + TextDF['label'].apply(str)\n",
    "TextDF = TextDF.pivot(index = ['label'], columns = ['level'], values = ['text']).reset_index()\n",
    "TextDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatType = 'SentenceAligned/'\n",
    "\n",
    "# Get file path for OneStopEnglish Corpus from individual TXT files.\n",
    "RawDat = os.getcwd() + '/data_raw/OneStopEng/' + DatType\n",
    "\n",
    "t_files = ['ADV-ELE.txt', 'ADV-INT.txt', 'ELE-INT.txt']\n",
    "\n",
    "for t in t_files:\n",
    "        \n",
    "    # Create labels depending on which file inported\n",
    "    if t == 'ADV-ELE.txt':\n",
    "        l1 = 'adv'\n",
    "        l2 = 'ele'\n",
    "    if t == 'ADV-INT.txt':\n",
    "        l1 = 'adv'\n",
    "        l2 = 'int'\n",
    "    if t == 'ELE-INT.txt':\n",
    "        l1 = 'ele'\n",
    "        l2 = 'int'\n",
    "        \n",
    "    with open(RawDat + t, 'rb') as rawdat:\n",
    "        result = chardet.detect(rawdat.read(10000))\n",
    "\n",
    "    file = open(RawDat + t, 'r', encoding = result['encoding'])\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    text = text.split('\\n')\n",
    "\n",
    "    TextDF = pd.DataFrame(text, columns = ['text'])\n",
    "    TextDF = TextDF[~TextDF['text'].isin(['*******', '', ' '])].reset_index().drop(columns = 'index')\n",
    "    TextDF['level'] = [l1, l2]*int(len(TextDF.index)/2)\n",
    "    TextDF['label'] = list(np.repeat(list(range(1, int(len(TextDF.index)/2)+1)), 2))\n",
    "    TextDF['label'] = 's' + TextDF['label'].apply(str)\n",
    "\n",
    "    # Pivot text to level\n",
    "    TextDF = TextDF.pivot(index = ['label'], columns = ['level'], values = ['text']).reset_index()\n",
    "\n",
    "    # Flatten column index of pivot table\n",
    "    TextDF.columns = TextDF.columns.map('_'.join).str.strip('_')\n",
    "    \n",
    "    # Create text variable in which more difficult sentence is above easier sentence, separated by |<endoflevelone>|\n",
    "    if t == 'ELE-INT.txt':\n",
    "        l1 = 'int'\n",
    "        l2 = 'ele'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    NewVar = l1+'_'+l2\n",
    "    TextDF[NewVar] = TextDF['text_'+l1] + '\\n |<endoflevelone>| \\n' + TextDF['text_'+l2]\n",
    "    TextDF = TextDF.drop_duplicates()\n",
    "    \n",
    "    # Export TextDF to csv file\n",
    "    TextDF[NewVar].to_csv(ClnDat+'OSE_'+NewVar+'_sent.csv', encoding = 'utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT USE--IMPORTS TEXT FILE USING PANDAS, AND SOME ROWS ARE MISALIGNED\n",
    "# THERE IS ALSO AN ISSUE LOADING THE 'ELE-INT' FILE USING PANDAS\n",
    "\n",
    "# Create cleaned sentence aligned files for text generation analysis\n",
    "\n",
    "# Data Type: Either \"TextByReadingLevel\" or \"SentenceAligned\"\n",
    "DatType = 'SentenceAligned/'\n",
    "\n",
    "# Get file path for OneStopEnglish Corpus from individual TXT files.\n",
    "RawDat = os.getcwd() + '/data_raw/OneStopEng/' + DatType\n",
    "\n",
    "t_files = ['ADV-ELE.txt', 'ADV-INT.txt', 'ELE-INT.txt']\n",
    "\n",
    "for t in t_files:\n",
    "    \n",
    "    # Read in txt file\n",
    "    TextDF = pd.read_csv(RawDat + t, sep = '\\n', header = None)\n",
    "    \n",
    "    # Create labels depending on which file inported\n",
    "    if t == 'ADV-ELE.txt':\n",
    "        l1 = 'adv'\n",
    "        l2 = 'ele'\n",
    "    if t == 'ADV-INT.txt':\n",
    "        l1 = 'adv'\n",
    "        l2 = 'int'\n",
    "    if t == 'ELE-INT.txt':\n",
    "        l1 = 'ele'\n",
    "        l2 = 'int'\n",
    "        \n",
    "    TextDF.columns = ['text']\n",
    "    TextDF['level'] = [l1, l2, 'split']*int(len(TextDF.index)/3)\n",
    "    TextDF['label'] = list(np.repeat(list(range(1, int(len(TextDF.index)/3)+1)), 3))\n",
    "    TextDF = TextDF[TextDF['level'] != 'split'].reset_index().drop(columns = ['index'])\n",
    "    TextDF['label'] = 's' + TextDF['label'].apply(str)\n",
    "\n",
    "    # Pivot text to level\n",
    "    TextDF = TextDF.pivot(index = ['label'], columns = ['level'], values = ['text']).reset_index()\n",
    "\n",
    "    # Flatten column index of pivot table\n",
    "    TextDF.columns = TextDF.columns.map('_'.join).str.strip('_')\n",
    "    \n",
    "    # Create text variable in which more difficult sentence is above easier sentence, separated by |<endoflevelone>|\n",
    "    if t == 'ELE-INT.txt':\n",
    "        l1 = 'int'\n",
    "        l2 = 'ele'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    NewVar = l1+'_'+l2\n",
    "    TextDF[NewVar] = TextDF['text_'+l1] + '\\n |<endoflevelone>| \\n' + TextDF['text_'+l2]\n",
    "    \n",
    "    # Export TextDF to csv file\n",
    "    TextDF[NewVar].to_csv(ClnDat+'OSE_'+NewVar+'_sent.csv', encoding = 'utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# IMPORT DATA STORED IN CSV FILES #\n",
    "###################################\n",
    "\n",
    "# Get file path to OneStopEnglish Corpus including combined CSV files.\n",
    "FilePath = os.getcwd() + '/OneStopEng/TextOneCSVPerFile/'\n",
    "\n",
    "# Texts are organized by difficulty level\n",
    "Files = listdir(FilePath)\n",
    "\n",
    "# Import all texts into pandas dataframe\n",
    "oseDF = pd.DataFrame() # Initialize dataframe\n",
    "\n",
    "# Loop through all files\n",
    "for f in Files:\n",
    "    \n",
    "    # Files have different encodings\n",
    "    # Find the encoding for each file and use that in read_csv command\n",
    "    with open(FilePath + f, 'rb') as rawdat:\n",
    "        result = chardet.detect(rawdat.read(10000))\n",
    "        \n",
    "    oseDFA = pd.read_csv(FilePath + f, encoding = result['encoding'])\n",
    "    oseDFA['name'] = f\n",
    "    \n",
    "    oseDF = oseDF.append(oseDFA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
