{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneStopEnglish: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import textstat\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets option to display all text in pandas dataframes\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path for OneStopEnglish Corpus from individual TXT files.\n",
    "FilePath = os.getcwd() + '/OneStopEng/TextByReadingLevel/'\n",
    "\n",
    "# For some reason there are two \"Int\" files--do not import the second\n",
    "SubDir = [join(FilePath, f)+'/' for f in listdir(FilePath) if f != 'Int2-Txt']\n",
    "\n",
    "# Load all texts into pandas data frame\n",
    "TextDF = pd.DataFrame()\n",
    "\n",
    "for s in SubDir:\n",
    "    TextFiles = listdir(s)\n",
    "\n",
    "    for t in TextFiles:\n",
    "        # There is a hidden .ds_store file that should be skipped when importing data\n",
    "        if t == '.DS_Store':\n",
    "            pass\n",
    "        else:\n",
    "            file = open(s + t, 'r')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "\n",
    "            TextDF = TextDF.append({'file_nm' : t,\n",
    "                                    'text' : text},\n",
    "                                  ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate name and level of text and drop file_nm\n",
    "TextDF['name'] = TextDF['file_nm'].str[:-8]\n",
    "TextDF['level'] = TextDF['file_nm'].str[-7:-4]\n",
    "TextDF['text'] = TextDF['text'].apply(str)\n",
    "\n",
    "# Intermediate text is labeled--this should be removed\n",
    "TextDF['text'] = TextDF['text'].map(lambda x: x.lstrip('Intermediate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute text difficulty using textstat\n",
    "TextDF['difficulty'] = TextDF['text'].apply(textstat.text_standard)\n",
    "# TextDF.groupby('level')['difficulty'].plot(kind = 'hist', legend = True)\n",
    "# TextDF.groupby(['level', 'difficulty'])['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiDF_sub['text'] = WikiDF_sub['text'].apply(str) # Turn text to string\n",
    "WikiDF_sub['fkg_score'] = WikiDF_sub['text'].apply(textstat.flesch_kincaid_grade)\n",
    "WikiDF_sub['flesch_read'] = WikiDF_sub['text'].apply(textstat.flesch_reading_ease)\n",
    "WikiDF_sub['fog_score'] = WikiDF_sub['text'].apply(textstat.gunning_fog)\n",
    "WikiDF_sub['ari_score'] = WikiDF_sub['text'].apply(textstat.automated_readability_index)\n",
    "WikiDF_sub['cli_score'] = WikiDF_sub['text'].apply(textstat.coleman_liau_index)\n",
    "WikiDF_sub['lwf_score'] = WikiDF_sub['text'].apply(textstat.linsear_write_formula)\n",
    "WikiDF_sub['dcr_score'] = WikiDF_sub['text'].apply(textstat.dale_chall_readability_score)\n",
    "WikiDF_sub['consensus'] = WikiDF_sub['text'].apply(textstat.text_standard)\n",
    "WikiDF_sub['n_sentences'] = WikiDF_sub['text'].apply(textstat.sentence_count)\n",
    "WikiDF_sub['n_syllables'] = WikiDF_sub['text'].apply(textstat.syllable_count)\n",
    "WikiDF_sub['n_lexicon'] = WikiDF_sub['text'].apply(textstat.lexicon_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path to OneStopEnglish Corpus including combined CSV files.\n",
    "FilePath = os.getcwd() + '/OneStopEng/TextOneCSVPerFile/'\n",
    "\n",
    "# Texts are organized by difficulty level\n",
    "Files = listdir(FilePath)\n",
    "\n",
    "# Import all texts into pandas dataframe\n",
    "oseDF = pd.DataFrame() # Initialize dataframe\n",
    "\n",
    "# Loop through all files\n",
    "for f in Files:\n",
    "    \n",
    "    # Files have different encodings\n",
    "    # Find the encoding for each file and use that in read_csv command\n",
    "    with open(FilePath + f, 'rb') as rawdat:\n",
    "        result = chardet.detect(rawdat.read(10000))\n",
    "        \n",
    "    oseDFA = pd.read_csv(FilePath + f, encoding = result['encoding'])\n",
    "    oseDFA['name'] = f\n",
    "    \n",
    "    oseDF = oseDF.append(oseDFA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
