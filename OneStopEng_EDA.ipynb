{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneStopEnglish: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import textstat\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets option to display all text in pandas dataframes\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path for OneStopEnglish Corpus from individual TXT files.\n",
    "FilePath = os.getcwd() + '/OneStopEng/TextByReadingLevel/'\n",
    "\n",
    "# For some reason there are two \"Int\" files--do not import the first, as there \n",
    "SubDir = [join(FilePath, f)+'/' for f in listdir(FilePath) if f != 'Int-Txt']\n",
    "\n",
    "# Load all texts into pandas data frame\n",
    "TextDF = pd.DataFrame()\n",
    "\n",
    "for s in SubDir:\n",
    "    TextFiles = listdir(s)\n",
    "\n",
    "    for t in TextFiles:\n",
    "        # There is a hidden .ds_store file that should be skipped when importing data\n",
    "        if t == '.DS_Store':\n",
    "            pass\n",
    "        else:\n",
    "            file = open(s + t, 'r')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "\n",
    "            # Append text data and file name to dataframe\n",
    "            TextDF = TextDF.append({'file_nm' : t,\n",
    "                                    'text' : text},\n",
    "                                   ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate name and level of text and drop file_nm\n",
    "TextDF['name'] = TextDF['file_nm'].str[:-8]\n",
    "TextDF['level'] = TextDF['file_nm'].str[-7:-4]\n",
    "TextDF['text'] = TextDF['text'].apply(str)\n",
    "\n",
    "TextDF.groupby(['level'])['text'].count()\n",
    "\n",
    "# Intermediate text is labeled--this should be removed\n",
    "# TextDF['text'] = TextDF['text'].map(lambda x: x.lstrip('Intermediate'))\n",
    "# TextDF['text'] = TextDF['text'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute text difficulty using textstat\n",
    "TextDF['difficulty'] = TextDF['text'].apply(textstat.flesch_reading_ease)\n",
    "\n",
    "# Compute different statistics for text entries\n",
    "TextDF['n_sent'] = TextDF['text'].apply(textstat.sentence_count)\n",
    "TextDF['n_syll'] = TextDF['text'].apply(textstat.syllable_count)\n",
    "TextDF['n_lex'] = TextDF['text'].apply(textstat.lexicon_count)\n",
    "TextDF['syll_sent'] = TextDF['n_syll']/TextDF['n_sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDF.groupby('level')['difficulty'].plot(kind = 'hist', legend = True, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so results can be reproduced\n",
    "randomseed = 20200922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "\n",
    "# Label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Define predictor variables\n",
    "PredVars = ['difficulty', 'n_lex', 'n_syll', 'n_sent']\n",
    "\n",
    "# Subset TextDF to desired variables\n",
    "MLDat = TextDF[['level']+PredVars].reset_index().drop(columns = ['index'])\n",
    "MLDat['level'] = le.fit_transform(MLDat['level'])\n",
    "\n",
    "# Target and predictor variables\n",
    "y = MLDat['level']\n",
    "X = MLDat[PredVars]\n",
    "\n",
    "# Split data set into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = randomseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes Model\n",
    "gnb = GaussianNB()\n",
    "pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, pred, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear support verctor classification\n",
    "svc = LinearSVC(random_state = randomseed)\n",
    "pred = svc.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, pred, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K neighbors classifier\n",
    "knc = KNeighborsClassifier(n_neighbors = 3)\n",
    "knc.fit(X_train, y_train)\n",
    "pred = knc.predict(X_test)\n",
    "accuracy_score(y_test, pred, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial logistic regression\n",
    "mlr = LogisticRegression(max_iter = 450)\n",
    "pred = mlr.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, pred, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# IMPORT DATA STORED IN CSV FILES #\n",
    "###################################\n",
    "\n",
    "# Get file path to OneStopEnglish Corpus including combined CSV files.\n",
    "FilePath = os.getcwd() + '/OneStopEng/TextOneCSVPerFile/'\n",
    "\n",
    "# Texts are organized by difficulty level\n",
    "Files = listdir(FilePath)\n",
    "\n",
    "# Import all texts into pandas dataframe\n",
    "oseDF = pd.DataFrame() # Initialize dataframe\n",
    "\n",
    "# Loop through all files\n",
    "for f in Files:\n",
    "    \n",
    "    # Files have different encodings\n",
    "    # Find the encoding for each file and use that in read_csv command\n",
    "    with open(FilePath + f, 'rb') as rawdat:\n",
    "        result = chardet.detect(rawdat.read(10000))\n",
    "        \n",
    "    oseDFA = pd.read_csv(FilePath + f, encoding = result['encoding'])\n",
    "    oseDFA['name'] = f\n",
    "    \n",
    "    oseDF = oseDF.append(oseDFA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
